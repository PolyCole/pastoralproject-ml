{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Maybe need to alter this to function with S3.\n",
    "path_to_file = \"poems.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 652476 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author,content,poem name,age,type\r\n",
      "WILLIAM SHAKESPEARE,\"Let the bird of loudest lay\r\n",
      "On the sole Arabian tree\r\n",
      "Herald sad and trumpet be,\r\n",
      "To whose sound chaste wings obey.\r\n",
      "\r\n",
      "But thou shrieking harbinger,\r\n",
      "Foul precurrer of the fiend,\r\n",
      "Augur of the \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYyNlCNXymwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '&' :   5,\n",
      "  \"'\" :   6,\n",
      "  '(' :   7,\n",
      "  ')' :   8,\n",
      "  ',' :   9,\n",
      "  '-' :  10,\n",
      "  '.' :  11,\n",
      "  '/' :  12,\n",
      "  '0' :  13,\n",
      "  '1' :  14,\n",
      "  '2' :  15,\n",
      "  '3' :  16,\n",
      "  '4' :  17,\n",
      "  '5' :  18,\n",
      "  '6' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'author,conten' ---- characters mapped to int ---- > [55 75 74 62 69 72  9 57 69 68 74 59 68]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "u\n",
      "t\n",
      "h\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'author,content,poem name,age,type\\r\\nWILLIAM SHAKESPEARE,\"Let the bird of loudest lay\\r\\nOn the sole Arab'\n",
      "'ian tree\\r\\nHerald sad and trumpet be,\\r\\nTo whose sound chaste wings obey.\\r\\n\\r\\nBut thou shrieking harbing'\n",
      "\"er,\\r\\nFoul precurrer of the fiend,\\r\\nAugur of the fever's end,\\r\\nTo this troop come thou not near.\\r\\n\\r\\nFr\"\n",
      "\"om this session interdict\\r\\nEvery fowl of tyrant wing,\\r\\nSave the eagle, feather'd king;\\r\\nKeep the obse\"\n",
      "'quy so strict.\\r\\n\\r\\nLet the priest in surplice white,\\r\\nThat defunctive music can,\\r\\nBe the death-divinin'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'author,content,poem name,age,type\\r\\nWILLIAM SHAKESPEARE,\"Let the bird of loudest lay\\r\\nOn the sole Ara'\n",
      "Target data: 'uthor,content,poem name,age,type\\r\\nWILLIAM SHAKESPEARE,\"Let the bird of loudest lay\\r\\nOn the sole Arab'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 55 ('a')\n",
      "  expected output: 75 ('u')\n",
      "Step    1\n",
      "  input: 75 ('u')\n",
      "  expected output: 74 ('t')\n",
      "Step    2\n",
      "  input: 74 ('t')\n",
      "  expected output: 62 ('h')\n",
      "Step    3\n",
      "  input: 62 ('h')\n",
      "  expected output: 69 ('o')\n",
      "Step    4\n",
      "  input: 69 ('o')\n",
      "  expected output: 72 ('r')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "  print(\"Step {:4d}\".format(i))\n",
    "  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "  print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           23552     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 92)            94300     \n",
      "=================================================================\n",
      "Total params: 4,056,156\n",
      "Trainable params: 4,056,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 50, 10, 49, 74, 78, 91, 14, 50,  8, 48, 16, 53,  4, 29, 70, 86,\n",
       "       69, 76, 69, 30, 19, 78,  7, 54, 65, 77, 73, 18, 83, 10, 86, 62, 22,\n",
       "        2, 15, 20, 27, 20,  1, 18, 23, 27, 14, 79,  6, 76, 24, 88, 90, 72,\n",
       "       74, 37, 35, 65, 60, 40, 48,  5, 55, 91, 26, 91, 79,  4, 65, 26, 30,\n",
       "        3, 81, 36, 32, 23, 26, 23, 74, 53, 53, 44, 78,  9,  5, 71, 69, 69,\n",
       "       68, 63, 82, 67, 64, 29, 86, 60, 64, 20, 67, 73, 51, 33, 55])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '\\r\\n\\r\\nPoetry Out Loud Note: In the print anthology, this poem is titled simply \"\"Song.\"\" The student m'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'GY-Xtx”1Y)W3]\"DpéovoE6x(_kws5â-éh9 27B7\\r5:B1y\\'v;—“rtLJkfOW&a”A”y\"kAE!{KG:A:t]]Sx,&qooni}mjDéfj7msZHa'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 92)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.52165\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 128s 1s/step - loss: 3.0435\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 122s 1s/step - loss: 2.2232\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 123s 1s/step - loss: 2.0000\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 123s 1s/step - loss: 1.8457\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 124s 1s/step - loss: 1.7341\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 124s 1s/step - loss: 1.6484\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 125s 1s/step - loss: 1.5778\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 136s 1s/step - loss: 1.5180\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 125s 1s/step - loss: 1.4642\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 124s 1s/step - loss: 1.4121\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            23552     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 92)             94300     \n",
      "=================================================================\n",
      "Total params: 4,056,156\n",
      "Trainable params: 4,056,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 5000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # We pass the predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afoot and lighthearted the heavenly bonew,\r\n",
      "Her that Here's nothing not so meet:\r\n",
      "And we proczed out out funtiens oft with atour schearted with so,\r\n",
      "Readered all wores durt did make,\r\n",
      "Whose postraised with the permission of Aughis Death.\",Te Darne,Modern,Mythology & Folklore\r\n",
      "EDGAR LET SIANDBURG,\"MyGutures, onl when we was a men accequia,Modern,Nature\r\n",
      "G. H. Liep, the wooking down them eyes\r\n",
      "            What is my life, nor thining Muses, retern Angulanciou, yet pipt? Ot is of turne,\r\n",
      "Such dark trans that some work as tre sit ahis,\r\n",
      "Me hostier lively edgent\r\n",
      "Pass in all mims.\r\n",
      "But whom so thy suffers?   \r\n",
      "that lonely give the lookes the field\r\n",
      "Hadder ththe winds flowing from the insurneds\r\n",
      "Seareful writing appouce\r\n",
      "The mither as a soul of fouch a tender will be in long:\r\n",
      "      And if thy morning we of the prone chief,\r\n",
      "And came sufficing churr of hir.\r\n",
      "Listen me no pure o'crold hearted to grown by Hemorge of evening in warring companion.\r\n",
      "For I still sir of knight,\r\n",
      "In Soverong Briedflaxis.\r\n",
      "I never new as us! you ask dark sav, weere, with hop will se;\r\n",
      "An imso stretches, on his heart buck. Re\r\n",
      "And if side, wilt poeces forth akitment,\r\n",
      "           come beniever but, that I have not exco\r\n",
      "Calls, to hig wind therefore eat rymess could boug\r\n",
      "Ere within he slam not never in the Water. Centrain light.\r\n",
      "\r\n",
      "Full is maketh of succears,\r\n",
      "How wait she merays stir to shades our looker sweetest,\r\n",
      "Might an wise, I am beft be doom.\r\n",
      "As, Wantine as the world, behome\r\n",
      "For thour mimes they gone up would to you, more ware\r\n",
      "            And we were went dead.,   1u, 1963 Gone, O Rouited Poems 1927-138] OF ENGLAND\"\", Phelape Against Vieching from Selected Poems of Groan,Modern,Nature\r\n",
      "WILLIAM SHAKESPEARE,\"Gurned blush unine,\r\n",
      "When well justily to thee, the esone, at lay!\r\n",
      "But went, It was once and stmading mournesse,\r\n",
      "Full descrive its cleed at le\r\n",
      "       He could not eas? A glad I see ever shall heard:\r\n",
      "Can her threw, sur-sing, which it embrabitically:There all deer blade,\r\n",
      "Remaning thee and beautys folk did hie,\r\n",
      "Like beauties felt chance doth stream.\r\n",
      "I make them long will, as unbestedles;\r\n",
      "Or white, durl me; might raises underled\r\n",
      "With being balls a view' have so love and slip eyes\r\n",
      "As dead snall he tuscent Pertuous sasmphatillay gaine,\r\n",
      "Unsongue wasse thy bough the dust up sighes.\r\n",
      "Ever so watch thrice friendsChopplace she broke,\r\n",
      "As write agieas return disgrace, hee pake now some with the little way,\r\n",
      "In anytsing slept seemes in thee: she as hart\r\n",
      "As could have loved cry\r\n",
      ": be wordful neither branch, and te\r\n",
      "T    To tember this great sas she makest,\r\n",
      "Even arment as suched courtles lagge,\r\n",
      "Who is to wete as new stream of meat,\r\n",
      "And with a holly with exgre: the post, and inoby realougd a blunk\r\n",
      "Of heavenly die againe the work-offections.\r\n",
      "On the shadom be distracies\r\n",
      "\r\n",
      "Did we turn woth time with Gailon.\r\n",
      "\r\n",
      "\r\n",
      "But when I was a imbort of prine?\r\n",
      "            Her body waits that hear my straok.\r\n",
      "But when temphere is my dogeterness hell braunched.\r\n",
      "But Chelly Setsest, I blesceth undernove?\r\n",
      "Lone of orcunce he marrigied loves me not: \r\n",
      "Thy soul if I would counge head with lies graze out dies,\r\n",
      "For unsure and a lovely mildies vive that is shall lands chaste doth stare\r\n",
      "Bullous as ne wimpin's arise.\",The Lady was expigring sevming backn!\r\n",
      "At he vells mournful sense, well current?\r\n",
      "Ang straight west with comercate my grace, and were of black;\r\n",
      "Franion before is laving falme, which these givesure me.\",Folle with wite a mindle ademnour blund,\r\n",
      "    Went ines dot decketh all thinslout the monelory retterngue it ours in hair.\r\n",
      "\r\n",
      "The bluck, that never; ne lest them at hove is line.\r\n",
      "                             He windowing; Oblone Lefe so longly\r\n",
      "Me with the resthe flock her went,\r\n",
      "All the wint tender gllances, well your boures away?\r\n",
      "Then ween I am blood nature:\r\n",
      "For I young moon both yourself wine and falst filt confeture:\r\n",
      "To you affection.\r\n",
      "Ne lie we safous sellom wixtom;   \r\n",
      "The coustral marrest ch,\r\n",
      "May night wishets mine eyes grieved force,\r\n",
      "\r\n",
      "For the sweet more in tears, and winds my way be myniven.\r\n",
      "\r\n",
      "More or trawe-breath,\r\n",
      "The still the winto wisting nemplers light.\r\n",
      "But in that like her love what was with unsued take\r\n",
      "To serving howe then we true, that not feed his westity's ling that ring\r\n",
      "Pharms I see hee love, and waste, and we are forten umon our serven, as waiting unterts,\r\n",
      "And makes me bridles long heavenly came,\r\n",
      "That she wise-praise of Love,Modern,Nature\r\n",
      "HOUIS UNTFet ARONIE,\"That tretic and its congring Venus,,Renaissance,Mythology & Folklore\r\n",
      "EDMULD SPEARE,\"Ot, some soptry, run\r\n",
      "   Is should in fine called and loves;\r\n",
      "And let the purrict vallwy briyfer of sakity:\r\n",
      "And long, O seeme we Bode is instances fexferded\r\n",
      "Of endice and fore of time who love deare.\r\n",
      "Then we for tiald once a face to more.\r\n",
      "\r\n",
      "The brains of the fieldly, imest,\r\n",
      "Huch with Gobbars swhence soveression of our wins, I dear,\r\n",
      "Doe not so, unto your selond sorry—time, good-sort,\r\n",
      "Follow with the flink-place praise a bird is deard,\r\n",
      "          Let first this to cleave.\r\n",
      "Wever as s\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Afoot and lighthearted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XAm7eCoKULT"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUKhnZtMVpoJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4kH1o0leVIp"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4tSNwymzf-q"
   },
   "outputs": [],
   "source": [
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # resetting the hidden state at the start of every epoch\n",
    "  model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}